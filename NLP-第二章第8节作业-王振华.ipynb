{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/2-5-autoencoder/\n",
    "https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726\n",
    "\n",
    "Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as code to the original input as possible.\n",
    "自编码是一种非监督型机器学习神经网路，它能学会高效地压缩和重建数据，让重建数据和原数据尽可能地一样。\n",
    "\n",
    "Autoencoder, by design, reduces data dimensions by learning how to ignore the noise in the data.\n",
    "\n",
    "有时神经网络的输入数据量非常大，如果直接用原数据，计算量会非常大。如果先用部分数据训练出一个自编码，然后用它来压缩所有原数据，再输入神经网络，就可以大大减小计算量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy search 贪心搜索，每次都选择概率最大的那个可能\n",
    "beam search 集束搜索，第一次选择概率最大的前n个（如3个、5个）可能，对每个可能都继续计算下去，选择最终这n个可能中概率最大的那个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention mechanism 是一种用于提升RNN的encoder + decoder 模型效果的机制，比如在机器翻译、语音识别中，为句子中的每个词赋予不同的权重，使模型更准确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一词多义问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双层双向的LSTM，对每一层的隐状态做一个加权平均，作为最终值\n",
    "\n",
    "其双向只是简单粗暴的拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://zhuanlan.zhihu.com/p/54743941\n",
    "\n",
    "RNN无法做并行计算，Transfomer可以做并行计算\n",
    "\n",
    "Transfomer 特征抽取能力更强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://zhuanlan.zhihu.com/p/38755603\n",
    "\n",
    "batch normalization 是对各个样本的一个维度做归一化，所以受batch size影响；\n",
    "\n",
    "layer normalization 是对一个样本的不同维度做归一化，不受 batch size影响，一个sample就可以做"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://zhuanlan.zhihu.com/p/54743941\n",
    "\n",
    "因为输入的第一层网络是 multi-head self attention 层，self attention 会让当前输入单词和句子中任意单词发生关系，然后集成到一个embedding向量里，但当所有信息到了embedding后，位置信息并没有被编码进去。所以，transformer 必须明确地在输入端，将position信息编码，即 position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://zhuanlan.zhihu.com/p/47282410\n",
    "\n",
    "self-Attention则利用了Attention机制，计算每个单词与其他所有单词之间的关联，这样就更能利用上下文信息来判断当前这个词。\n",
    "\n",
    "Multi-head Attention其实就是多个Self-Attention结构的结合，每个head学习到在不同表示空间中的特征，多个head学习到的Attention侧重点可能略有不同，这样给了模型更大的容量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于 transformer 的decoder，是多层单向的transformer\n",
    "\n",
    "参考：https://blog.csdn.net/qq_33373858/article/details/89479038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://blog.csdn.net/qq_33373858/article/details/89479038\n",
    "\n",
    "GPT的核心思想是先通过无标签的文本去训练生成语言模型，再根据具体的NLP任务（如文本蕴涵、QA、文本分类等），来通过有标签的数据对模型进行fine-tuning。\n",
    "\n",
    "具体来说，结合了无监督的预训练和有监督的fine-tuning。\n",
    "首先，在未标记数据集上训练语言模型来学习神经网络模型的初始参数。\n",
    "随后，使用相应NLP任务中的有标签的数据将这些参数微调，来适应当前任务。\n",
    "\n",
    "参考：https://zhuanlan.zhihu.com/p/49271699\n",
    "\n",
    "下游任务要向GPT的网络结构看齐，把任务的网络结构改造成和GPT的网络结构是一样的。\n",
    "分类问题，加上一个起始和终止符号即可；\n",
    "句子关系判断问题，比如entailment，两个句子中间再加个分隔符即可；\n",
    "文本相似性判断问题，把两个句子顺序颠倒下做出两个输入，以告诉模型句子顺序不重要；\n",
    "多项选择问题，则多路输入，每一路把文章和答案选项拼接作为输入。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考： https://tech.meituan.com/2019/11/14/nlp-bert-practice.html\n",
    "\n",
    "本质思想类似于CBOW，但是细节方面有改进。\n",
    "随机选择语料中15%的单词，用[Mask]掩码代替原始单词，然后预测这些被掩盖的词来训练双向语言模型，并使每个词的表征参考上下文信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对不同的任务，bert模型的输入可以是单句或句对。\n",
    "对于每一个输入的token，它的表征由其对应的词表征(token embedding)、句表征(segment embedding)和位置表征(position embedding)相加产生"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briefly descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分析，只需要在最终的 CLS + Linear\n",
    "\n",
    "词性标注，则需要使用每个输出\n",
    "\n",
    "参考：https://zhuanlan.zhihu.com/p/49271699\n",
    "\n",
    "对于句子关系类任务，和GPT类似，加上一个起始和终结符号，句子之间加个分隔符即可。对于输出来说，把第一个起始符号对应的Transformer最后一层位置上面串接一个softmax分类层即可。\n",
    "\n",
    "对于分类问题，与GPT一样，只需要增加起始和终结符号，输出部分和句子关系判断任务类似改造；\n",
    "\n",
    "对于序列标注问题，输入部分和单句分类是一样的，只需要输出部分Transformer最后一层每个单词对应位置都进行分类即可。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 是单向的transformer，GPT2只是训练数据量更大\n",
    "\n",
    "BERT 是双向的transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
